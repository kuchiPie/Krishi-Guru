{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Wikipedia articles for search\n",
    "\n",
    "This notebook shows how we prepared a dataset of Wikipedia articles for search, used in [Question_answering_using_embeddings.ipynb](Question_answering_using_embeddings.ipynb).\n",
    "\n",
    "Procedure:\n",
    "\n",
    "0. Prerequisites: Import libraries, set API key (if needed)\n",
    "1. Collect: We download a few hundred Wikipedia articles about the 2022 Olympics\n",
    "2. Chunk: Documents are split into short, semi-self-contained sections to be embedded\n",
    "3. Embed: Each section is embedded with the OpenAI API\n",
    "4. Store: Embeddings are saved in a CSV file (for large datasets, use a vector database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import mwclient  # for downloading example Wikipedia articles\n",
    "import mwparserfromhell  # for splitting Wikipedia articles into sections\n",
    "import openai  # for generating embeddings\n",
    "import pandas as pd  # for DataFrames to store article sections and embeddings\n",
    "import re  # for cutting <ref> links out of Wikipedia articles\n",
    "import tiktoken  # for counting tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting mwclient\n",
      "  Downloading mwclient-0.10.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: requests-oauthlib in /home/siddharthb/.local/lib/python3.10/site-packages (from mwclient) (1.3.1)\n",
      "Requirement already satisfied: six in /home/siddharthb/.local/lib/python3.10/site-packages (from mwclient) (1.12.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/siddharthb/.local/lib/python3.10/site-packages (from requests-oauthlib->mwclient) (3.2.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /home/siddharthb/.local/lib/python3.10/site-packages (from requests-oauthlib->mwclient) (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib->mwclient) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/siddharthb/.local/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib->mwclient) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/siddharthb/.local/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib->mwclient) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/siddharthb/.local/lib/python3.10/site-packages (from requests>=2.0.0->requests-oauthlib->mwclient) (2.8)\n",
      "Installing collected packages: mwclient\n",
      "Successfully installed mwclient-0.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install mwclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting mwparserfromhell\n",
      "  Downloading mwparserfromhell-0.6.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (190 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m221.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m193.1 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mwparserfromhell\n",
      "Successfully installed mwparserfromhell-0.6.4\n"
     ]
    }
   ],
   "source": [
    "!pip install mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in /home/siddharthb/.local/lib/python3.10/site-packages (0.27.2)\n",
      "Requirement already satisfied: tqdm in /home/siddharthb/.local/lib/python3.10/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /home/siddharthb/.local/lib/python3.10/site-packages (from openai) (3.8.3)\n",
      "Requirement already satisfied: requests>=2.20 in /home/siddharthb/.local/lib/python3.10/site-packages (from openai) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/siddharthb/.local/lib/python3.10/site-packages (from requests>=2.20->openai) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3.10/site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/siddharthb/.local/lib/python3.10/site-packages (from requests>=2.20->openai) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/siddharthb/.local/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.14)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/siddharthb/.local/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/siddharthb/.local/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/siddharthb/.local/lib/python3.10/site-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/siddharthb/.local/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/siddharthb/.local/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/siddharthb/.local/lib/python3.10/site-packages (from aiohttp->openai) (22.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tiktoken in /home/siddharthb/.local/lib/python3.10/site-packages (0.3.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/lib/python3.10/site-packages (from tiktoken) (2023.3.23)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/siddharthb/.local/lib/python3.10/site-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/siddharthb/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/siddharthb/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/siddharthb/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install any missing libraries with `pip install` in your terminal. E.g.,\n",
    "\n",
    "```zsh\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "(You can also do this in a notebook cell with `!pip install openai`.)\n",
    "\n",
    "If you install any libraries, be sure to restart the notebook kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set API key (if needed)\n",
    "\n",
    "Note that the OpenAI library will try to read your API key from the `OPENAI_API_KEY` environment variable. If you haven't already, set this environment variable by following [these instructions](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collect documents\n",
    "\n",
    "In this example, we'll download a few hundred Wikipedia articles related to the 2022 Winter Olympics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1787 article titles in Category:Agriculture.\n"
     ]
    }
   ],
   "source": [
    "# get Wikipedia pages about the 2022 Winter Olympics\n",
    "\n",
    "CATEGORY_TITLE = \"Category:Agriculture\"\n",
    "WIKI_SITE = \"en.wikipedia.org\"\n",
    "\n",
    "\n",
    "def titles_from_category(\n",
    "    category: mwclient.listing.Category, max_depth: int\n",
    ") -> set[str]:\n",
    "    \"\"\"Return a set of page titles in a given Wiki category and its subcategories.\"\"\"\n",
    "    titles = set()\n",
    "    for cm in category.members():\n",
    "        if type(cm) == mwclient.page.Page:\n",
    "            # ^type() used instead of isinstance() to catch match w/ no inheritance\n",
    "            titles.add(cm.name)\n",
    "        elif isinstance(cm, mwclient.listing.Category) and max_depth > 0:\n",
    "            deeper_titles = titles_from_category(cm, max_depth=max_depth - 1)\n",
    "            titles.update(deeper_titles)\n",
    "    return titles\n",
    "\n",
    "\n",
    "site = mwclient.Site(WIKI_SITE)\n",
    "category_page = site.pages[CATEGORY_TITLE]\n",
    "titles = titles_from_category(category_page, max_depth=1)\n",
    "# ^note: max_depth=1 means we go one level deep in the category tree\n",
    "print(f\"Found {len(titles)} article titles in {CATEGORY_TITLE}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chunk documents\n",
    "\n",
    "Now that we have our reference documents, we need to prepare them for search.\n",
    "\n",
    "Because GPT can only read a limited amount of text at once, we'll split each document into chunks short enough to be read.\n",
    "\n",
    "For this specific example on Wikipedia articles, we'll:\n",
    "- Discard less relevant-looking sections like External Links and Footnotes\n",
    "- Clean up the text by removing reference tags (e.g., <ref>), whitespace, and super short sections\n",
    "- Split each article into sections\n",
    "- Prepend titles and subtitles to each section's text, to help GPT understand the context\n",
    "- If a section is long (say, > 1,600 tokens), we'll recursively split it into smaller sections, trying to split along semantic boundaries like paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to split Wikipedia pages into sections\n",
    "\n",
    "SECTIONS_TO_IGNORE = [\n",
    "    \"See also\",\n",
    "    \"References\",\n",
    "    \"External links\",\n",
    "    \"Further reading\",\n",
    "    \"Footnotes\",\n",
    "    \"Bibliography\",\n",
    "    \"Sources\",\n",
    "    \"Citations\",\n",
    "    \"Literature\",\n",
    "    \"Footnotes\",\n",
    "    \"Notes and references\",\n",
    "    \"Photo gallery\",\n",
    "    \"Works cited\",\n",
    "    \"Photos\",\n",
    "    \"Gallery\",\n",
    "    \"Notes\",\n",
    "    \"References and sources\",\n",
    "    \"References and notes\",\n",
    "]\n",
    "\n",
    "\n",
    "def all_subsections_from_section(\n",
    "    section: mwparserfromhell.wikicode.Wikicode,\n",
    "    parent_titles: list[str],\n",
    "    sections_to_ignore: set[str],\n",
    ") -> list[tuple[list[str], str]]:\n",
    "    \"\"\"\n",
    "    From a Wikipedia section, return a flattened list of all nested subsections.\n",
    "    Each subsection is a tuple, where:\n",
    "        - the first element is a list of parent subtitles, starting with the page title\n",
    "        - the second element is the text of the subsection (but not any children)\n",
    "    \"\"\"\n",
    "    headings = [str(h) for h in section.filter_headings()]\n",
    "    title = headings[0]\n",
    "    if title.strip(\"=\" + \" \") in sections_to_ignore:\n",
    "        # ^wiki headings are wrapped like \"== Heading ==\"\n",
    "        return []\n",
    "    titles = parent_titles + [title]\n",
    "    full_text = str(section)\n",
    "    section_text = full_text.split(title)[1]\n",
    "    if len(headings) == 1:\n",
    "        return [(titles, section_text)]\n",
    "    else:\n",
    "        first_subtitle = headings[1]\n",
    "        section_text = section_text.split(first_subtitle)[0]\n",
    "        results = [(titles, section_text)]\n",
    "        for subsection in section.get_sections(levels=[len(titles) + 1]):\n",
    "            results.extend(all_subsections_from_section(subsection, titles, sections_to_ignore))\n",
    "        return results\n",
    "\n",
    "\n",
    "def all_subsections_from_title(\n",
    "    title: str,\n",
    "    sections_to_ignore: set[str] = SECTIONS_TO_IGNORE,\n",
    "    site_name: str = WIKI_SITE,\n",
    ") -> list[tuple[list[str], str]]:\n",
    "    \"\"\"From a Wikipedia page title, return a flattened list of all nested subsections.\n",
    "    Each subsection is a tuple, where:\n",
    "        - the first element is a list of parent subtitles, starting with the page title\n",
    "        - the second element is the text of the subsection (but not any children)\n",
    "    \"\"\"\n",
    "    site = mwclient.Site(site_name)\n",
    "    page = site.pages[title]\n",
    "    text = page.text()\n",
    "    parsed_text = mwparserfromhell.parse(text)\n",
    "    headings = [str(h) for h in parsed_text.filter_headings()]\n",
    "    if headings:\n",
    "        summary_text = str(parsed_text).split(headings[0])[0]\n",
    "    else:\n",
    "        summary_text = str(parsed_text)\n",
    "    results = [([title], summary_text)]\n",
    "    for subsection in parsed_text.get_sections(levels=[2]):\n",
    "        results.extend(all_subsections_from_section(subsection, [title], sections_to_ignore))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Houston black (soil)\n",
      "Comprehensive Assessment of Water Management in Agriculture\n",
      "Fallow\n",
      "Pitsilia\n",
      "Continuous harvest\n",
      "Ration stamp\n",
      "Pitchfork\n",
      "Organic fertilizer\n",
      "Coffee bean\n",
      "List of pollen sources\n",
      "Martin County Fair\n",
      "Sheldon Farms\n",
      "Horticultural building system\n",
      "Bell pepper\n",
      "Adverse Effect Wage Rate\n",
      "Palm oil\n",
      "List of agricultural deities\n",
      "Biodiversity in agriculture\n",
      "Wadi Rum Consultancy\n",
      "Field (agriculture)\n",
      "Neglected and underutilized crop\n",
      "Van Diemen's Land Company\n",
      "Permanent Interstate Committee for drought control in the Sahel\n",
      "Bio-geoengineering\n",
      "Rootstock\n",
      "Animal Health Act 1981\n",
      "Nokken, Copenhagen\n",
      "Soil compaction (agriculture)\n",
      "Northern vigor\n",
      "Fall Plowing\n",
      "Huayuri\n",
      "Ogongo Agricultural College\n",
      "Western silvereye\n",
      "Sapporo Agricultural College\n",
      "Soil fertility\n",
      "Åsetesrett\n",
      "Vernalization\n",
      "Ant garden\n",
      "NatureSweet\n",
      "Agrivoltaics\n",
      "Agricultural engineering\n",
      "Cultipacker\n",
      "BioFach\n",
      "Orujo (olive waste)\n",
      "Strip farming\n",
      "Kumpula Allotment Garden\n",
      "Intercropping\n",
      "Fruit tree pollination\n",
      "Wood industry\n",
      "Ostrich farming in Namibia\n",
      "GTRI Agricultural Technology Research Program\n",
      "Slaughterhouse\n",
      "Agro-town\n",
      "Agroecology in West Africa\n",
      "Sams Plantation Complex Tabby Ruins\n",
      "Ñadi\n",
      "Otranto Plantation\n",
      "Meloidogyne thamesi\n",
      "Value-added agriculture\n",
      "Clethodim\n",
      "Hermetic storage\n",
      "Porsche Super\n",
      "Gleaning\n",
      "Haymaking in the Auvergne\n",
      "Sickle-gloss\n",
      "Packing house\n",
      "Reaper\n",
      "Plants in space\n",
      "Family farm\n",
      "Compartmentalization of decay in trees\n",
      "Chang'e 4\n",
      "Grain trade\n",
      "Tritucap\n",
      "Field day (agriculture)\n",
      "Compascuus\n",
      "Stable\n",
      "The Gleaners\n",
      "Multiple cropping\n",
      "Avian influenza\n",
      "Stubble-mulching\n",
      "Energy efficiency in agriculture\n",
      "Branch collar\n",
      "List of countries by cereal production\n",
      "Growing season\n",
      "Sugarcane\n",
      "Fairfield Materials Management Ltd\n",
      "Contract farming\n",
      "Growbag\n",
      "Andrew O. Jackson\n",
      "Rural tenancy\n",
      "Isles, Inc.\n",
      "Sub-irrigated planter\n",
      "Long-Term Agroecosystem Research Network\n",
      "Food-feed system\n",
      "Tanganyika groundnut scheme\n",
      "Heterodera cardiolata\n",
      "History of organic farming\n",
      "Haymaking (painting)\n",
      "Agronomy for Sustainable Development\n",
      "Hydroseeding\n",
      "Farm income\n",
      "Digital agriculture\n",
      "Bibliography of hedges and topiary\n",
      "Roundpole fence\n",
      "Cotton wool\n",
      "Bandwin\n",
      "Masao Kishimoto\n",
      "Agrarian society\n",
      "Microponics\n",
      "Companion planting\n",
      "Hometalk\n",
      "Mid-Somerset Show\n",
      "The Agrarian History of England and Wales\n",
      "Adolf von Liebenberg\n",
      "Farm museum\n",
      "Chili pepper\n",
      "Arab Agricultural Revolution\n",
      "Growstones\n",
      "Set-aside\n",
      "Elsinoë veneta\n",
      "Phytogeomorphology\n",
      "Organopónicos\n",
      "Teff\n",
      "AgCam\n",
      "List of diseases of the honey bee\n",
      "Animal pound\n",
      "Agricultural expansion\n",
      "Lettuce\n",
      "Falling Number\n",
      "National Ploughing Championships\n",
      "Multifunctionality in agriculture\n",
      "Physiological plant disorder\n",
      "Evesham Custom\n",
      "The Cornfield\n",
      "Elsenburg Agricultural Training Institute\n",
      "Open-field system\n",
      "Indigenous horticulture\n",
      "Coconut production in Indonesia\n",
      "Indaziflam\n",
      "Gene stacking\n",
      "Agricultural philosophy\n",
      "Stereo fruiting\n",
      "Natural rubber\n",
      "Drum chopper\n",
      "Lists of useful plants\n",
      "Richmond Plantation\n",
      "Inter-Regional Research Project Number 4\n",
      "Pomato\n",
      "Plant factory\n",
      "Fladry\n",
      "Eggatron\n",
      "Incredible Edible\n",
      "Steam tractor\n",
      "Livery yard\n",
      "Plant agriculture\n",
      "Tacarpo\n",
      "Post-harvest losses (vegetables)\n",
      "Biosphere 2\n",
      "High country (New Zealand)\n",
      "List of Roman agricultural deities\n",
      "Goatskin (material)\n",
      "Rice transplanter\n",
      "Bokashi (horticulture)\n",
      "Richard de Quincey\n",
      "Norwegian Agrarian Association\n",
      "Controlled traffic farming\n",
      "Harvest festival\n",
      "Rice Research Station, Moncombu\n",
      "Township (Scotland)\n",
      "Cromwell on his Farm\n",
      "Pyruvate scale\n",
      "CARBAP\n",
      "Landsupport\n",
      "Barahnaja\n",
      "Transplanting\n",
      "Coir\n",
      "Jimador\n",
      "Northumberland County Show\n",
      "History of cotton\n",
      "Restricted use pesticide\n",
      "AgWeatherNet\n",
      "Commodity Credit Corporation\n",
      "Canadian Grain Commission\n",
      "Precision agriculture\n",
      "Twin-scaling\n",
      "Masoveria\n",
      "National Farmers' Union of Scotland\n",
      "Kangchu system\n",
      "Organic horticulture\n",
      "Urban beekeeping\n",
      "Grain\n",
      "Convertible husbandry\n",
      "Tree topping\n",
      "Crop rotation\n",
      "Agricultural diversification\n",
      "Buffer initiative\n",
      "Sickle\n",
      "Veterinary pharmacy\n",
      "Reapers Resting in a Wheat Field\n",
      "Detroit Black Community Food Security Network\n",
      "Pastoralism\n",
      "Progeny testing\n",
      "Sri Lankan irrigation network\n",
      "Poultry farming\n",
      "Landless Workers' Movement\n",
      "Religion and agriculture\n",
      "Corn exchanges in England\n",
      "Conserving use acreage\n",
      "RediRipe\n",
      "CREP\n",
      "Controlled-environment agriculture\n",
      "Jay Laurence Lush\n",
      "Barn\n",
      "Livestock keepers' rights\n",
      "Agricultural technology\n",
      "List of vineyard soil types\n",
      "Gross processing margin\n",
      "Fur Farming (Prohibition) (Scotland) Act 2002\n",
      "Biofumigation\n",
      "Golf Course Allotments\n",
      "MIFEE\n",
      "Vincenzo Tanara\n",
      "National Prize for Applied Sciences and Technologies (Chile)\n",
      "United Nations Decade of Family Farming\n",
      "Green Tobacco Sickness\n",
      "Bailey Colony Farm\n",
      "October (painting)\n",
      "Corn-rent\n",
      "Shieling\n",
      "Hopper hut\n",
      "Alimenta\n",
      "Pannage\n",
      "Walle Plough\n",
      "Diseases of Animals Act\n",
      "Heterodera bergeniae\n",
      "Short rotation forestry\n",
      "Texas and Southwestern Cattle Raisers Association\n",
      "Cabaña pasiega\n",
      "HeHalutz\n",
      "Hay Harvest at Éragny\n",
      "Agriculture Innovation Center\n",
      "Paddy field\n",
      "Agrarian distress\n",
      "Gorru\n",
      "Corn dolly\n",
      "Agricultural landscape of southern Öland\n",
      "Off-farm income\n",
      "The Wheat Field\n",
      "Field corn\n",
      "Aurora Golden Gala\n",
      "Mulch-till\n",
      "Surfing Goat Dairy\n",
      "Soybean management practices\n",
      "Water sprout\n",
      "Hermetia illucens\n",
      "EQ-53\n",
      "Cowspiracy\n",
      "GreenWave\n",
      "Charles Newbold\n",
      "Summer fallow\n",
      "Can You Dig This\n",
      "Raised field\n",
      "Weed of cultivation\n",
      "CropSyst\n",
      "AgDevCo\n",
      "Tomato grafting\n",
      "Domestication of the sheep\n",
      "Pernil Alto\n",
      "Christianity and agriculture\n",
      "Henry Thompson (veterinary surgeon)\n",
      "Topiary\n",
      "Agricultural road\n",
      "Permaculture\n",
      "Cattle urine patches\n",
      "IMARK\n",
      "Dibotryon morbosum\n",
      "Horse collar\n",
      "Contract for future sale\n",
      "Ploughing match\n",
      "African Journal of Range & Forage Science\n",
      "Headland (agriculture)\n",
      "Cover crop\n",
      "List of inedible fruits\n",
      "Honduras Foundation for Agricultural Research\n",
      "Goat tower\n",
      "Sty\n",
      "International Association of Agricultural Economists\n",
      "Wheat\n",
      "Agro-terrorism\n",
      "Sedaxane\n",
      "Siwa Oasis\n",
      "Green famine\n",
      "Food, Inc. (book)\n",
      "Tangerine\n",
      "Frost resistance\n",
      "Kötter\n",
      "Hügelkultur\n",
      "Amish Paste\n",
      "Captive bolt pistol\n",
      "Strategic grain reserve\n",
      "List of crop plants pollinated by bees\n",
      "Groundcover\n",
      "Forum for Agricultural Research in Africa\n",
      "Leaf Bank\n",
      "Arboriculture\n",
      "Alpe-Adria\n",
      "Masters in Agricultural Economics\n",
      "Slash-and-char\n",
      "Paraguayan Agricultural Development\n",
      "Sexing\n",
      "Royal Highland and Agricultural Society of Scotland\n",
      "Agriculturist\n",
      "Cultural control\n",
      "Equivalence (trade)\n",
      "Extensive farming\n",
      "List of farms in Cornwall\n",
      "Grainfield at the Edge of a Wood\n",
      "Participatory disease surveillance\n",
      "DeWitt Clinton Park\n",
      "Anguina (nematode)\n",
      "Spraing\n",
      "National Agricultural Policy Center\n",
      "Corncob\n",
      "Agricultural chemistry\n",
      "Policy Issues\n",
      "List of countries by artichoke production\n",
      "Anguina spermophaga\n",
      "McKenzie Seeds\n",
      "Winter cereal\n",
      "Digital Dera\n",
      "Tsou Ma Lai Farm\n",
      "Agriculture in the Middle Ages\n",
      "Compost Everything\n",
      "John Deere DB120\n",
      "Ornamental bulbous plant\n",
      "Anguina amsinckiae\n",
      "Metribuzin\n",
      "Peri-urban agriculture\n",
      "Hay steaming\n",
      "Xenogamy\n",
      "IR8\n",
      "Grain entrapment\n",
      "Paleoethnobotany of the Mapuche\n",
      "Agronomy\n",
      "Experimental forest\n",
      "Enclosure\n",
      "Agriculture in Mesoamerica\n",
      "Village Roots Garden\n",
      "Cereal\n",
      "Meanwood Valley Urban Farm\n",
      "Fruit picking\n",
      "Cattle prod\n",
      "Spindly growth\n",
      "Birch-bark roof\n",
      "Deep water culture\n",
      "Threshing stone\n",
      "Sharecropping\n",
      "Peasant Women with Brushwood\n",
      "Heliciculture\n",
      "Nepalese cocoyam\n",
      "Waterlogging (agriculture)\n",
      "Grist\n",
      "Animal Health and Welfare Act 1984\n",
      "Central Administration of Plant Quarantine\n",
      "Marquis wheat\n",
      "Agricultural and Rural Convention 2020\n",
      "Posted county price\n",
      "Barn raising\n",
      "Hay rack\n",
      "Round barn\n",
      "Food Information and Control Agency\n",
      "Abdul Khader Nadakattin\n",
      "Peak wheat\n",
      "Gradan\n",
      "Dam (agricultural reservoir)\n",
      "Ecuador composting method\n",
      "Riverton Site\n",
      "Schachten\n",
      "Urban agriculture\n",
      "Spermospora avenae\n",
      "Landesque capital\n",
      "Small farm\n",
      "Newington Plantation\n",
      "Girna\n",
      "Food industry\n",
      "Dead hedge\n",
      "Farm Credit Act of 1933\n",
      "Foot-and-mouth disease\n",
      "Sindhri\n",
      "Perryman Ranch Headquarters\n",
      "Community gardening in the United States\n",
      "Dessert crop\n",
      "Whip (tree)\n",
      "Punjab Agri Export Corporation Limited\n",
      "Ancient Egyptian agriculture\n",
      "Alex Avery\n",
      "Carroll Place\n",
      "Gordon McClymont\n",
      "Drought tolerance\n",
      "Agricultural spiritualism\n",
      "Zemědělská ekonomika\n",
      "List of countries by tomato production\n",
      "Subsistence agriculture\n",
      "The Peasants Returning From The Fields\n",
      "Wine cave\n",
      "Arable land\n",
      "Areca nut production in India\n",
      "Incan agriculture\n"
     ]
    },
    {
     "ename": "ReadTimeout",
     "evalue": "HTTPSConnectionPool(host='en.wikipedia.org', port=443): Read timed out. (read timeout=30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m/usr/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/util/retry.py:550\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 550\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/packages/six.py:770\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 770\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:451\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:340\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m timeout_value\n\u001b[1;32m    342\u001b[0m     )\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3. In Python 2 we have\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# to specifically catch it and throw the timeout error\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='en.wikipedia.org', port=443): Read timed out. (read timeout=30)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m wikipedia_sections \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m title \u001b[38;5;129;01min\u001b[39;00m titles:\n\u001b[0;32m----> 5\u001b[0m     wikipedia_sections\u001b[38;5;241m.\u001b[39mextend(\u001b[43mall_subsections_from_title\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(title)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(wikipedia_sections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sections in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(titles)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m pages.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 67\u001b[0m, in \u001b[0;36mall_subsections_from_title\u001b[0;34m(title, sections_to_ignore, site_name)\u001b[0m\n\u001b[1;32m     65\u001b[0m site \u001b[38;5;241m=\u001b[39m mwclient\u001b[38;5;241m.\u001b[39mSite(site_name)\n\u001b[1;32m     66\u001b[0m page \u001b[38;5;241m=\u001b[39m site\u001b[38;5;241m.\u001b[39mpages[title]\n\u001b[0;32m---> 67\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m parsed_text \u001b[38;5;241m=\u001b[39m mwparserfromhell\u001b[38;5;241m.\u001b[39mparse(text)\n\u001b[1;32m     69\u001b[0m headings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(h) \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m parsed_text\u001b[38;5;241m.\u001b[39mfilter_headings()]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mwclient/page.py:157\u001b[0m, in \u001b[0;36mPage.text\u001b[0;34m(self, section, expandtemplates, cache, slot)\u001b[0m\n\u001b[1;32m    154\u001b[0m revs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrevisions(prop\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent|timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, section\u001b[38;5;241m=\u001b[39msection,\n\u001b[1;32m    155\u001b[0m                       slots\u001b[38;5;241m=\u001b[39mslot)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     rev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrevs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslots\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m rev:\n\u001b[1;32m    159\u001b[0m         text \u001b[38;5;241m=\u001b[39m rev[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslots\u001b[39m\u001b[38;5;124m'\u001b[39m][slot][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mwclient/listing.py:61\u001b[0m, in \u001b[0;36mList.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast:\n\u001b[1;32m     60\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m item:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mwclient/listing.py:325\u001b[0m, in \u001b[0;36mRevisionsIterator.load_chunk\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrvstartid\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrvstart\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrvstart\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRevisionsIterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mwclient/listing.py:95\u001b[0m, in \u001b[0;36mList.load_chunk\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;124;03m\"\"\"Query a new chunk of data\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    If the query is empty, `raise StopIteration`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    Else, set `self.last` to True.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miteritems\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;66;03m# Non existent page\u001b[39;00m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mwclient/client.py:234\u001b[0m, in \u001b[0;36mSite.get\u001b[0;34m(self, action, *args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, action, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;124;03m\"\"\"Perform a generic API call using GET.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    This is just a shorthand for calling api() with http_method='GET'.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m        The raw response from the API call, as a dictionary.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mwclient/client.py:285\u001b[0m, in \u001b[0;36mSite.api\u001b[0;34m(self, action, http_method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m sleeper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msleepers\u001b[38;5;241m.\u001b[39mmake()\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m info:\n\u001b[1;32m    287\u001b[0m         info \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mwclient/client.py:437\u001b[0m, in \u001b[0;36mSite.raw_api\u001b[0;34m(self, action, http_method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    436\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_string(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 437\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mapi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_on_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mhttp_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_method\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(res, object_pairs_hook\u001b[38;5;241m=\u001b[39mOrderedDict)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mwclient/client.py:399\u001b[0m, in \u001b[0;36mSite.raw_call\u001b[0;34m(self, script, data, files, retry_on_error, http_method)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m--> 399\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx-database-lag\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    402\u001b[0m     wait_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(stream\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretry-after\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:578\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[0;32m--> 578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidHeader(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mReadTimeout\u001b[0m: HTTPSConnectionPool(host='en.wikipedia.org', port=443): Read timed out. (read timeout=30)"
     ]
    }
   ],
   "source": [
    "# split pages into sections\n",
    "# may take ~1 minute per 100 articles\n",
    "wikipedia_sections = []\n",
    "for title in titles:\n",
    "    wikipedia_sections.extend(all_subsections_from_title(title))\n",
    "    print(title)\n",
    "print(f\"Found {len(wikipedia_sections)} sections in {len(titles)} pages.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2309"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wikipedia_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 125 sections, leaving 2184 sections.\n"
     ]
    }
   ],
   "source": [
    "# clean text\n",
    "def clean_section(section: tuple[list[str], str]) -> tuple[list[str], str]:\n",
    "    \"\"\"\n",
    "    Return a cleaned up section with:\n",
    "        - <ref>xyz</ref> patterns removed\n",
    "        - leading/trailing whitespace removed\n",
    "    \"\"\"\n",
    "    titles, text = section\n",
    "    text = re.sub(r\"<ref.*?</ref>\", \"\", text)\n",
    "    text = text.strip()\n",
    "    return (titles, text)\n",
    "\n",
    "\n",
    "wikipedia_sections = [clean_section(ws) for ws in wikipedia_sections]\n",
    "\n",
    "# filter out short/blank sections\n",
    "def keep_section(section: tuple[list[str], str]) -> bool:\n",
    "    \"\"\"Return True if the section should be kept, False otherwise.\"\"\"\n",
    "    titles, text = section\n",
    "    if len(text) < 16:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "original_num_sections = len(wikipedia_sections)\n",
    "wikipedia_sections = [ws for ws in wikipedia_sections if keep_section(ws)]\n",
    "print(f\"Filtered out {original_num_sections-len(wikipedia_sections)} sections, leaving {len(wikipedia_sections)} sections.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Houston black (soil)']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"'''Houston black soil''' extends over {{convert|1500000|acre|km2}} of the [[T...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Comprehensive Assessment of Water Management in Agriculture']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{{Italic title}}\\n{{Infobox organization\\n|name  = Water for food, Water for li...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Comprehensive Assessment of Water Management in Agriculture', '==History==']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Compiled after consultation with more than 700 individuals, numerous organisa...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Comprehensive Assessment of Water Management in Agriculture', '==Trends affecting demands for water==']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The report's authors forecast that the need for water would double within 50 ...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['Comprehensive Assessment of Water Management in Agriculture', '==How feeding the future world will be possible==']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[[File:Corn pivot irrigated.jpg|thumb|right|[[Irrigation]] water management i...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# print example data\n",
    "for ws in wikipedia_sections[:5]:\n",
    "    print(ws[0])\n",
    "    display(ws[1][:77] + \"...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll recursively split long sections into smaller sections.\n",
    "\n",
    "There's no perfect recipe for splitting text into sections.\n",
    "\n",
    "Some tradeoffs include:\n",
    "- Longer sections may be better for questions that require more context\n",
    "- Longer sections may be worse for retrieval, as they may have more topics muddled together\n",
    "- Shorter sections are better for reducing costs (which are proportional to the number of tokens)\n",
    "- Shorter sections allow more sections to be retrieved, which may help with recall\n",
    "- Overlapping sections may help prevent answers from being cut by section boundaries\n",
    "\n",
    "Here, we'll use a simple approach and limit sections to 1,000 tokens each, recursively halving any sections that are too long. To avoid cutting in the middle of useful sentences, we'll split along paragraph boundaries when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_MODEL = \"gpt-3.5-turbo\"  # only matters insofar as it selects which tokenizer to use\n",
    "\n",
    "\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
    "    \"\"\"Split a string in two, on a delimiter, trying to balance tokens on each side.\"\"\"\n",
    "    chunks = string.split(delimiter)\n",
    "    if len(chunks) == 1:\n",
    "        return [string, \"\"]  # no delimiter found\n",
    "    elif len(chunks) == 2:\n",
    "        return chunks  # no need to search for halfway point\n",
    "    else:\n",
    "        total_tokens = num_tokens(string)\n",
    "        halfway = total_tokens // 2\n",
    "        best_diff = halfway\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            left = delimiter.join(chunks[: i + 1])\n",
    "            left_tokens = num_tokens(left)\n",
    "            diff = abs(halfway - left_tokens)\n",
    "            if diff >= best_diff:\n",
    "                break\n",
    "            else:\n",
    "                best_diff = diff\n",
    "        left = delimiter.join(chunks[:i])\n",
    "        right = delimiter.join(chunks[i:])\n",
    "        return [left, right]\n",
    "\n",
    "\n",
    "def truncated_string(\n",
    "    string: str,\n",
    "    model: str,\n",
    "    max_tokens: int,\n",
    "    print_warning: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"Truncate a string to a maximum number of tokens.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    encoded_string = encoding.encode(string)\n",
    "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
    "    if print_warning and len(encoded_string) > max_tokens:\n",
    "        print(f\"Warning: Truncated string from {len(encoded_string)} tokens to {max_tokens} tokens.\")\n",
    "    return truncated_string\n",
    "\n",
    "\n",
    "def split_strings_from_subsection(\n",
    "    subsection: tuple[list[str], str],\n",
    "    max_tokens: int = 1000,\n",
    "    model: str = GPT_MODEL,\n",
    "    max_recursion: int = 5,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Split a subsection into a list of subsections, each with no more than max_tokens.\n",
    "    Each subsection is a tuple of parent titles [H1, H2, ...] and text (str).\n",
    "    \"\"\"\n",
    "    titles, text = subsection\n",
    "    string = \"\\n\\n\".join(titles + [text])\n",
    "    num_tokens_in_string = num_tokens(string)\n",
    "    # if length is fine, return string\n",
    "    if num_tokens_in_string <= max_tokens:\n",
    "        return [string]\n",
    "    # if recursion hasn't found a split after X iterations, just truncate\n",
    "    elif max_recursion == 0:\n",
    "        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
    "    # otherwise, split in half and recurse\n",
    "    else:\n",
    "        titles, text = subsection\n",
    "        for delimiter in [\"\\n\\n\", \"\\n\", \". \"]:\n",
    "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
    "            if left == \"\" or right == \"\":\n",
    "                # if either half is empty, retry with a more fine-grained delimiter\n",
    "                continue\n",
    "            else:\n",
    "                # recurse on each half\n",
    "                results = []\n",
    "                for half in [left, right]:\n",
    "                    half_subsection = (titles, half)\n",
    "                    half_strings = split_strings_from_subsection(\n",
    "                        half_subsection,\n",
    "                        max_tokens=max_tokens,\n",
    "                        model=model,\n",
    "                        max_recursion=max_recursion - 1,\n",
    "                    )\n",
    "                    results.extend(half_strings)\n",
    "                return results\n",
    "    # otherwise no split was found, so just truncate (should be very rare)\n",
    "    return [truncated_string(string, model=model, max_tokens=max_tokens)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2184 Wikipedia sections split into 2217 strings.\n"
     ]
    }
   ],
   "source": [
    "# split sections into chunks\n",
    "MAX_TOKENS = 1600\n",
    "wikipedia_strings = []\n",
    "for section in wikipedia_sections:\n",
    "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
    "\n",
    "print(f\"{len(wikipedia_sections)} Wikipedia sections split into {len(wikipedia_strings)} strings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprehensive Assessment of Water Management in Agriculture\n",
      "\n",
      "{{Italic title}}\n",
      "{{Infobox organization\n",
      "|name  = Water for food, Water for life: A Comprehensive Assessment of Water Management in Agriculture\n",
      "|image = Comprehensive assessment cover.jpeg\n",
      "|type = Independent scientific assessment\n",
      "|publisher = IWMI, Earthscan (2007)\n",
      "|website = {{URL|www.iwmi.cgiar.org/assessment}}\n",
      "}}\n",
      "The report '''''A Comprehensive Assessment of Water Management in Agriculture''''' was published in 2007 by [[International Water Management Institute]] and [[Earthscan]] in an attempt to answer the question: how can [[water]] in [[agriculture]] be developed and managed to help end [[poverty]] and [[hunger]], ensure environmentally sustainable practices, and find the right balance between food and environmental security?\n"
     ]
    }
   ],
   "source": [
    "# print example data\n",
    "print(wikipedia_strings[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embed document chunks\n",
    "\n",
    "Now that we've split our library into shorter self-contained strings, we can compute embeddings for each.\n",
    "\n",
    "(For large embedding jobs, use a script like [api_request_parallel_processor.py](api_request_parallel_processor.py) to parallelize requests while throttling to stay under rate limits.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 to 999\n",
      "Batch 1000 to 1999\n",
      "Batch 2000 to 2999\n"
     ]
    }
   ],
   "source": [
    "# calculate embeddings\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "openai.api_key = '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'# OpenAI's best embeddings as of Apr 2023\n",
    "BATCH_SIZE = 1000  # you can submit up to 2048 embedding inputs per request\n",
    "\n",
    "embeddings = []\n",
    "for batch_start in range(0, len(wikipedia_strings), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = wikipedia_strings[batch_start:batch_end]\n",
    "    print(f\"Batch {batch_start} to {batch_end-1}\")\n",
    "    response = openai.Embedding.create(model=EMBEDDING_MODEL, input=batch)\n",
    "    for i, be in enumerate(response[\"data\"]):\n",
    "        assert i == be[\"index\"]  # double check embeddings are in same order as input\n",
    "    batch_embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
    "    embeddings.extend(batch_embeddings)\n",
    "\n",
    "df = pd.DataFrame({\"text\": wikipedia_strings, \"embedding\": embeddings})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Store document chunks and embeddings\n",
    "\n",
    "Because this example only uses a few thousand strings, we'll store them in a CSV file.\n",
    "\n",
    "(For larger datasets, use a vector database, which will be more performant.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save document chunks and embeddings\n",
    "\n",
    "SAVE_PATH = \"embddings.csv\"\n",
    "\n",
    "df.to_csv(SAVE_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
